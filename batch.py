#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡ç”ŸæˆQAå¯¹çš„è„šæœ¬
è‡ªåŠ¨æ›´æ¢URLä¸­çš„æ–‡ä»¶åï¼Œæ‰¹é‡å¤„ç†å¤šä¸ªID
"""

import json
import os
import time
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('config.env')

# ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
API_KEY = os.getenv('OPENAI_API_KEY')

def generate_image_urls(data_id: str) -> list:
    """
    æ ¹æ®æ•°æ®IDç”Ÿæˆå¯¹åº”çš„å›¾ç‰‡URLåˆ—è¡¨
    ç›´æ¥æ‰«æoriginal_data/imageç›®å½•æ‰¾åˆ°å®é™…å­˜åœ¨çš„æ–‡ä»¶
    
    Args:
        data_id: æ•°æ®IDï¼Œå¦‚ "0001", "0002" ç­‰
        
    Returns:
        åŒ…å«4å¼ å›¾ç‰‡URLçš„åˆ—è¡¨
    """
    base_url = "https://raw.githubusercontent.com/liyanlin06/any2any_data/main/general_area/food/image"
    
    image_urls = []
    
    # æ‰«æoriginal_data/imageç›®å½•ï¼Œæ‰¾åˆ°æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
    image_dir = "original_data/image"
    
    for i in range(1, 5):  # ç”Ÿæˆ4å¼ å›¾ç‰‡çš„URL
        filename_pattern = f"img_{data_id}_{i:02d}"
        
        # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶ï¼ˆæ”¯æŒä»»ä½•æ‰©å±•åï¼‰
        found_file = None
        for file in os.listdir(image_dir):
            if file.startswith(filename_pattern) and '.' in file:
                found_file = file
                break
        
        if found_file:
            # æ„å»ºå®Œæ•´çš„URL
            image_url = f"{base_url}/{found_file}"
            image_urls.append(image_url)
            print(f"      ğŸ“ æ‰¾åˆ°æ–‡ä»¶: {found_file}")
        else:
            # å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤æ‰©å±•å
            default_ext = 'jpg' if i == 1 else 'jpeg'
            image_url = f"{base_url}/{filename_pattern}.{default_ext}"
            image_urls.append(image_url)
            print(f"      âš ï¸  æœªæ‰¾åˆ°æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤: {filename_pattern}.{default_ext}")
    
    return image_urls

def call_openai_api(prompt: str, image_urls: list) -> str:
    """
    è°ƒç”¨OpenAI APIï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥
    
    Args:
        prompt: å‘é€çš„æ–‡æœ¬prompt
        image_urls: å›¾ç‰‡URLåˆ—è¡¨
        
    Returns:
        APIè¿”å›çš„å“åº”
    """
    client = OpenAI(api_key=API_KEY)
    
    try:
        # æ„å»ºæ¶ˆæ¯å†…å®¹ï¼ŒåŒ…å«æ–‡æœ¬å’Œå›¾ç‰‡
        content = [{"type": "text", "text": prompt}]
        
        # æ·»åŠ å›¾ç‰‡URLåˆ°æ¶ˆæ¯å†…å®¹ä¸­
        for url in image_urls[:4]:  # æœ€å¤š4å¼ å›¾ç‰‡
            content.append({
                "type": "image_url", 
                "image_url": {"url": url}
            })
        
        response = client.chat.completions.create(
            model="gpt-4o",  # GPT-4o æ¨¡å‹
            messages=[
                {
                    "role": "system", 
                    "content": "You are a multimodal expert. Generate structured JSON data for multimodal question-answer pairs. Always respond with properly formatted, multi-line JSON that is easy to read."
                },
                {
                    "role": "user", 
                    "content": content
                }
            ],
            max_tokens=5000,
            response_format={"type": "json_object"}  # å¼ºåˆ¶è¾“å‡ºJSONæ ¼å¼
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"âŒ APIè°ƒç”¨å¤±è´¥: {str(e)}")
        return None

def create_prompt(image_urls: list, data_index: int = 1) -> str:
    """
    åˆ›å»ºæ‚¨æä¾›çš„prompt
    
    Args:
        image_urls: å›¾ç‰‡URLåˆ—è¡¨
        data_index: æ•°æ®ç´¢å¼•
        
    Returns:
        å®Œæ•´çš„prompt
    """
    # æ„å»ºåŸå§‹æ•°æ®
    original_data = {}
    for i, url in enumerate(image_urls[:4]):  # æœ€å¤š4å¼ å›¾ç‰‡
        original_data[f"image{i+1}"] = url
    
    prompt = f"""You are a multimodal expert. Based on the following original data, please construct a data (Question-Answer pair) entry that strictly conforms to the JSON format below.
Please design a multimodal interleaved Question-Answer pair. You can place different pieces of information from the original data into the input or output of the Question-Answer pair.

[Original data]
{json.dumps(original_data, indent=2, ensure_ascii=False)}

[Question-Answer pair JSON template]
This Question-Answer pair must adhere to the following structure in the following JSON template and don't generate additional information.
{{
    "domain": "general_domain",
    "subdomain": "food",
    "id": "{data_index}",
    "input": {{
        "modal": {{
            "image1": "url",
            "image2": "url",
            "image3": "url",
            "image4": "url"
        }},
        "content": "Must interleave <image1>, <image2>, <image3>, <image4> tags at the appropriate position in the text and clearly indicate that the numbers of audios needed in the answer to support or illustrate the explanation. For example, the answer must include n audios in the output to support or illustrate the explanation."
    }},
    "output": {{
        "modal": {{
            "audio1": "text",
            ...
        }},
        "content": "This is the golden annotation answer that the model is expected to generate. Your answer text MUST naturally integrate all <audio> tags at appropriate positions with the format of <audio1>, <audio2>, etc. within the text. IMPORTANT: All audios together should provide comprehensive explanation for ALL images, with each audio potentially focusing on different subsets of images. The content should reference each audio using <audio1>, <audio2>, etc. format to create a cohesive narrative."
    }}
}}

[Construction requirements]
1 You need to design appropriate question-answer pair and clearly indicate in the question which specific modalities other than text are required to be included in the answer.
2 The output content MUST reference all audio tags using <audio1>, <audio2>, etc. format to create a cohesive narrative.
3 The output modal (audio text) MUST reference all image tags using <image1>, <image2>, <image3>, <image4> format to indicate which images each audio explains. 
4 The content of the input is the entire input fed into the model. The question-answer pair should be open-world QA. 
5 The content of the input is the entire input fed into the model and the content of the output is the golden output of the model. You should design the input content and output content based on the original data.
6 The content of the input MUST contain all tags corresponding to the input modality using <image1>, <image2>, <image3>, <image4> format, and the content of the output MUST contain all tags corresponding to the output modality using <audio1>, <audio2>, etc. format.
7 Give the JSON directly, no additional output information.
8 The <> tags should be the components of the text sentence, not just a single word. For example, the <> tags can serve as the subject, object, or other components of the sentence.
9 Please note that the <> tags of the input should not appear in the output.
10 You should design the text of the <audio> and fill the text into the "text" position of the <audio>. Each audio text MUST reference the specific images it explains using <image1>, <image2>, <image3>, <image4> tags.
11 IMPORTANT: The number of <audio> in the output is 2. If the number of <audio> is more than 1, each audio can focus on different images, but all audios together must cover ALL images.
12 IMPORTANT: The output must contain exactly the same number of audios as specified in the input question with the STRICT format of <audio1>, <audio2>, etc. Remember: audios collectively explain all images, with each audio potentially focusing on different subsets. 
13 IMPORTANT: The output content MUST not contain the <image> tags of images in the output modal. The output content MUST reference all audio tags using <audio1>, <audio2>, etc. format STRICTLY to create a cohesive narrative that ties together all the audio explanations.
14 IMPORTANT: All audios together MUST cover ALL images (<image1>, <image2>, <image3>, <image4>). Each individual audio can focus on a subset of images, but collectively they must explain all 4 images.

[CRITICAL STYLE REQUIREMENTS]
- The audio text should be written in a natural, flowing narrative style, NOT in a mechanical or template-like format
- AVOID starting audio descriptions with phrases like "In the audio, the dish in <image1> is..." or "Moving to <image2>..." 
- Instead, write the audio text as if it's a natural, engaging description that flows smoothly from one image to another
- Use varied sentence structures and natural transitions between images
- The audio should sound like it's coming from a knowledgeable food expert giving a natural explanation, not from a rigid template

[Example of GOOD audio style (like 0001):]
"The dishes presented in the images highlight diverse culinary traditions. Beginning with <image1>, this hot pot showcases a variety of ingredients simmering together, reflecting communal dining practices commonly seen in East Asian cultures. Moving on to <image2>, the noodle dish emphasizes the simplicity and freshness of vegetables combined with noodles, a staple in many Asian cuisines..."

[Example of BAD audio style (like 0002):]
"In the audio, the dish in <image1> is a hot pot featuring tofu, mushrooms, and corn, highlighting its nourishing qualities. Moving to <image2>, the noodles served with vegetables create a hearty and warming meal..."

[IMPORTANT CLARIFICATION]
The number of audios (e.g., 2 or 3) indicates how many detailed audio explanations you should provide. Each audio can focus on different images, but ALL audios together must comprehensively cover ALL 4 images. For example:
- If you need 2 audios: Audio1 explains images 1&2, Audio2 explains images 3&4 (covers all 4 images)
- If you need 3 audios: Audio1 explains image 1, Audio2 explains images 2&3, Audio3 explains image 4 (covers all 4 images but the distribution of the images can be random)

[Example of correct understanding]
Question asks about 4 images and requests 2 audios â†’ Answer should describe ALL 4 images across BOTH audios combined, not necessarily each audio covering all images.

[Output Content Requirement]
The output content (golden annotation answer) MUST reference all audio tags using <audio1>, <audio2>, etc. format to create a cohesive narrative that connects all the audio explanations together.

[Output Modal Requirement]
The output modal (audio text) MUST reference all image tags using <image1>, <image2>, <image3>, <image4> format to indicate which specific images each audio explains. This ensures clear mapping between audios and images.

Please respond with only the JSON, no additional text or explanation.


    """

    return prompt

def process_single_id(data_id: str, delay: int = 2) -> bool:
    """
    å¤„ç†å•ä¸ªIDçš„QAå¯¹ç”Ÿæˆ
    
    Args:
        data_id: æ•°æ®IDï¼Œå¦‚ "0001"
        delay: APIè°ƒç”¨é—´éš”å»¶è¿Ÿ(ç§’)
        
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    print(f"\nğŸ”„ å¤„ç†ID: {data_id}")
    
    # ç”Ÿæˆå›¾ç‰‡URL
    image_urls = generate_image_urls(data_id)
    
    print(f"   ğŸ“· å›¾ç‰‡URLs:")
    for i, url in enumerate(image_urls, 1):
        print(f"      image{i}: {url}")
    
    # åˆ›å»ºprompt
    prompt = create_prompt(image_urls, data_id)
    
    # è°ƒç”¨API
    print(f"   ğŸ“¤ è°ƒç”¨OpenAI API...")
    response = call_openai_api(prompt, image_urls)
    
    if response:
        try:
            # æ¸…ç†å“åº”
            cleaned_response = response.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]
            cleaned_response = cleaned_response.strip()
            
            qa_pair = json.loads(cleaned_response)
            
            # ä¿å­˜åˆ°JSONLæ–‡ä»¶
            output_file = "generated_qa_pairs.jsonl"
            with open(output_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(qa_pair, ensure_ascii=False, indent=2) + "\n")
            
            print(f"   âœ… æˆåŠŸç”ŸæˆQAå¯¹ï¼Œå·²ä¿å­˜")
            return True
            
        except json.JSONDecodeError as e:
            print(f"   âŒ JSONè§£æå¤±è´¥: {e}")
            # ä¿å­˜åŸå§‹å“åº”åˆ°é”™è¯¯æ—¥å¿—
            with open("error_log.txt", "a", encoding="utf-8") as f:
                f.write(f"ID: {data_id}\nError: {e}\nResponse: {response}\n{'='*50}\n")
            return False
            
    else:
        print(f"   âŒ APIè°ƒç”¨å¤±è´¥")
        # è®°å½•é”™è¯¯
        with open("error_log.txt", "a", encoding="utf-8") as f:
            f.write(f"ID: {data_id}\nError: APIè°ƒç”¨å¤±è´¥\n{'='*50}\n")
        return False

def batch_process(start_id: int = 1, end_id: int = 10, delay: int = 2):
    """
    æ‰¹é‡å¤„ç†å¤šä¸ªID
    
    Args:
        start_id: èµ·å§‹ID
        end_id: ç»“æŸID
        delay: APIè°ƒç”¨é—´éš”å»¶è¿Ÿ(ç§’)
    """
    print("ğŸš€ FoodieQA æ‰¹é‡ç”ŸæˆQAå¯¹è„šæœ¬")
    print("=" * 50)
    print(f"ğŸ¯ å¤„ç†èŒƒå›´: ID {start_id:04d} åˆ° {end_id:04d}")
    print(f"â±ï¸  APIå»¶è¿Ÿ: {delay}ç§’")
    print("=" * 50)
    
    # æ£€æŸ¥APIå¯†é’¥
    if not API_KEY:
        print("âŒ è¯·åœ¨config.envæ–‡ä»¶ä¸­è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "generated_qa_data"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    success_count = 0
    error_count = 0
    
    # æ‰¹é‡å¤„ç†
    for i in range(start_id, end_id + 1):
        data_id = f"{i:04d}"  # æ ¼å¼åŒ–ä¸º4ä½æ•°å­—ï¼Œå¦‚ "0001"
        
        success = process_single_id(data_id, delay)
        if success:
            success_count += 1
        else:
            error_count += 1
        
        # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
        if i < end_id:
            print(f"   â³ ç­‰å¾…{delay}ç§’åç»§ç»­...")
            time.sleep(delay)
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*50}")
    print(f"ğŸ“Š å¤„ç†å®Œæˆ!")
    print(f"âœ… æˆåŠŸ: {success_count} ä¸ª")
    print(f"âŒ å¤±è´¥: {error_count} ä¸ª")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: generated_qa_pairs.jsonl")
    if error_count > 0:
        print(f"ğŸ“ é”™è¯¯æ—¥å¿—: error_log.txt")
    print(f"{'='*50}")

def main():
    """ä¸»å‡½æ•°"""
    print("FoodieQA æ‰¹é‡ç”ŸæˆQAå¯¹è„šæœ¬")
    print("=" * 50)
    print("è¯·é€‰æ‹©æ¨¡å¼:")
    print("1. å•æ¬¡å¤„ç† (æµ‹è¯•ç”¨)")
    print("2. æ‰¹é‡å¤„ç†")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1æˆ–2): ").strip()
    
    if choice == "1":
        # å•æ¬¡å¤„ç†æ¨¡å¼
        print("\nğŸ¯ å•æ¬¡å¤„ç†æ¨¡å¼")
        data_id = input("è¯·è¾“å…¥è¦å¤„ç†çš„ID (ä¾‹å¦‚: 0001): ").strip()
        if not data_id:
            data_id = "0001"
        
        process_single_id(data_id)
        
    elif choice == "2":
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        print("\nğŸ“Š æ‰¹é‡å¤„ç†æ¨¡å¼")
        start_id = input("è¯·è¾“å…¥èµ·å§‹ID (é»˜è®¤: 1): ").strip()
        end_id = input("è¯·è¾“å…¥ç»“æŸID (é»˜è®¤: 10): ").strip()
        delay = input("è¯·è¾“å…¥APIå»¶è¿Ÿç§’æ•° (é»˜è®¤: 2): ").strip()
        
        # è®¾ç½®é»˜è®¤å€¼
        start_id = int(start_id) if start_id.isdigit() else 1
        end_id = int(end_id) if end_id.isdigit() else 10
        delay = int(delay) if delay.isdigit() else 2
        
        batch_process(start_id, end_id, delay)
        
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main()