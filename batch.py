#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ÊâπÈáèÁîüÊàêQAÂØπÁöÑËÑöÊú¨
Ëá™Âä®Êõ¥Êç¢URL‰∏≠ÁöÑÊñá‰ª∂ÂêçÔºåÊâπÈáèÂ§ÑÁêÜÂ§ö‰∏™ID
"""

import json
import os
import time
from dotenv import load_dotenv
from openai import OpenAI

# Âä†ËΩΩÁéØÂ¢ÉÂèòÈáè
load_dotenv('config.env')

# ‰ªéÁéØÂ¢ÉÂèòÈáèËé∑ÂèñAPIÂØÜÈí•
API_KEY = os.getenv('OPENAI_API_KEY')

def generate_image_urls(data_id: str) -> list:
    """
    Ê†πÊçÆÊï∞ÊçÆIDÁîüÊàêÂØπÂ∫îÁöÑÂõæÁâáURLÂàóË°®
    Áõ¥Êé•Êâ´Êèèoriginal_data/imageÁõÆÂΩïÊâæÂà∞ÂÆûÈôÖÂ≠òÂú®ÁöÑÊñá‰ª∂
    
    Args:
        data_id: Êï∞ÊçÆIDÔºåÂ¶Ç "0001", "0002" Á≠â
        
    Returns:
        ÂåÖÂê´4Âº†ÂõæÁâáURLÁöÑÂàóË°®
    """
    base_url = "https://raw.githubusercontent.com/liyanlin06/any2any_data/main/general_area/food/image"
    
    image_urls = []
    
    # Êâ´Êèèoriginal_data/imageÁõÆÂΩïÔºåÊâæÂà∞ÊâÄÊúâÂåπÈÖçÁöÑÊñá‰ª∂
    image_dir = "original_data/image"
    
    for i in range(1, 5):  # ÁîüÊàê4Âº†ÂõæÁâáÁöÑURL
        filename_pattern = f"img_{data_id}_{i:02d}"
        
        # Êü•ÊâæÂåπÈÖçÁöÑÊñá‰ª∂ÔºàÊîØÊåÅ‰ªª‰ΩïÊâ©Â±ïÂêçÔºâ
        found_file = None
        for file in os.listdir(image_dir):
            if file.startswith(filename_pattern) and '.' in file:
                found_file = file
                break
        
        if found_file:
            # ÊûÑÂª∫ÂÆåÊï¥ÁöÑURL
            image_url = f"{base_url}/{found_file}"
            image_urls.append(image_url)
            print(f"      üìÅ ÊâæÂà∞Êñá‰ª∂: {found_file}")
        else:
            # Â¶ÇÊûúÊâæ‰∏çÂà∞Êñá‰ª∂Ôºå‰ΩøÁî®ÈªòËÆ§Êâ©Â±ïÂêç
            default_ext = 'jpg' if i == 1 else 'jpeg'
            image_url = f"{base_url}/{filename_pattern}.{default_ext}"
            image_urls.append(image_url)
            print(f"      ‚ö†Ô∏è  Êú™ÊâæÂà∞Êñá‰ª∂Ôºå‰ΩøÁî®ÈªòËÆ§: {filename_pattern}.{default_ext}")
    
    return image_urls

def call_openai_api(prompt: str, image_urls: list) -> str:
    """
    Ë∞ÉÁî®OpenAI APIÔºåÊîØÊåÅÂ§öÊ®°ÊÄÅËæìÂÖ•
    
    Args:
        prompt: ÂèëÈÄÅÁöÑÊñáÊú¨prompt
        image_urls: ÂõæÁâáURLÂàóË°®
        
    Returns:
        APIËøîÂõûÁöÑÂìçÂ∫î
    """
    client = OpenAI(api_key=API_KEY)
    
    try:
        # ÊûÑÂª∫Ê∂àÊÅØÂÜÖÂÆπÔºåÂåÖÂê´ÊñáÊú¨ÂíåÂõæÁâá
        content = [{"type": "text", "text": prompt}]
        
        # Ê∑ªÂä†ÂõæÁâáURLÂà∞Ê∂àÊÅØÂÜÖÂÆπ‰∏≠
        for url in image_urls[:4]:  # ÊúÄÂ§ö4Âº†ÂõæÁâá
            content.append({
                "type": "image_url", 
                "image_url": {"url": url}
            })
        
        response = client.chat.completions.create(
            model="gpt-4o",  # GPT-4o Ê®°Âûã
            messages=[
                {
                    "role": "system", 
                    "content": "You are a multimodal expert. Generate structured JSON data for multimodal question-answer pairs. Always respond with properly formatted, multi-line JSON that is easy to read."
                },
                {
                    "role": "user", 
                    "content": content
                }
            ],
            max_tokens=5000,
            response_format={"type": "json_object"}  # Âº∫Âà∂ËæìÂá∫JSONÊ†ºÂºè
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"‚ùå APIË∞ÉÁî®Â§±Ë¥•: {str(e)}")
        return None

def create_prompt(image_urls: list, data_index: int = 1) -> str:
    """
    ÂàõÂª∫ÊÇ®Êèê‰æõÁöÑprompt
    
    Args:
        image_urls: ÂõæÁâáURLÂàóË°®
        data_index: Êï∞ÊçÆÁ¥¢Âºï
        
    Returns:
        ÂÆåÊï¥ÁöÑprompt
    """
    # ÊûÑÂª∫ÂéüÂßãÊï∞ÊçÆ
    original_data = {}
    for i, url in enumerate(image_urls[:4]):  # ÊúÄÂ§ö4Âº†ÂõæÁâá
        original_data[f"image{i+1}"] = url
    
    prompt = f"""You are a multimodal expert. Based on the following original data, please construct a data (Question-Answer pair) entry that strictly conforms to the JSON format below.
Please design a multimodal interleaved Question-Answer pair. You can place different pieces of information from the original data into the input or output of the Question-Answer pair.

[Original data]
{json.dumps(original_data, indent=2, ensure_ascii=False)}

[Question-Answer pair JSON template]
This Question-Answer pair must adhere to the following structure in the following JSON template and don't generate additional information.
{{
    "domain": "general_domain",
    "subdomain": "food",
    "id": "{data_index}",
    "input": {{
        "modal": {{
            "image1": "url",
            "image2": "url",
            "image3": "url",
            "image4": "url"
        }},
        "content": "Must interleave <image1>, <image2>, <image3>, <image4> tags at the appropriate position in the text and clearly indicate that the numbers of audios needed in the answer to support or illustrate the explanation. For example, the answer must include n audios in the output to support or illustrate the explanation."
    }},
    "output": {{
        "modal": {{
            "audio1": "text",
            ...
        }},
        "content": "This is the golden annotation answer that the model is expected to generate. Your answer text MUST naturally integrate all <audio> tags at appropriate positions with the format of <audio1>, <audio2>, etc. within the text. IMPORTANT: All audios together should provide comprehensive explanation for ALL images, with each audio potentially focusing on different subsets of images. The content should reference each audio using <audio1>, <audio2>, etc. format to create a cohesive narrative."
    }}
}}

[Construction requirements]
1 You need to design appropriate question-answer pair and clearly indicate in the question which specific modalities other than text are required to be included in the answer.
2 The output content MUST reference all audio tags using <audio1>, <audio2>, etc. format to create a cohesive narrative.
3 The output modal (audio text) MUST reference all image tags using <image1>, <image2>, <image3>, <image4> format to indicate which images each audio explains. 
4 The content of the input is the entire input fed into the model. The question-answer pair should be open-world QA. 
5 The content of the input is the entire input fed into the model and the content of the output is the golden output of the model. You should design the input content and output content based on the original data.
6 The content of the input MUST contain all tags corresponding to the input modality using <image1>, <image2>, <image3>, <image4> format, and the content of the output MUST contain all tags corresponding to the output modality using <audio1>, <audio2>, etc. format.
7 Give the JSON directly, no additional output information.
8 The <> tags should be the components of the text sentence, not just a single word. For example, the <> tags can serve as the subject, object, or other components of the sentence.
9 Please note that the <> tags of the input should not appear in the output.
10 You should design the text of the <audio> and fill the text into the "text" position of the <audio>. Each audio text MUST reference the specific images it explains using <image1>, <image2>, <image3>, <image4> tags.
11 IMPORTANT: The number of <audio> in the output is 2. If the number of <audio> is more than 1, each audio can focus on different images, but all audios together must cover ALL images.
12 IMPORTANT: The output must contain exactly the same number of audios as specified in the input question with the STRICT format of <audio1>, <audio2>, etc. Remember: audios collectively explain all images, with each audio potentially focusing on different subsets. 
13 IMPORTANT: The output content MUST not contain the <image> tags of images in the output modal. The output content MUST reference all audio tags using <audio1>, <audio2>, etc. format STRICTLY to create a cohesive narrative that ties together all the audio explanations.
14 IMPORTANT: All audios together MUST cover ALL images (<image1>, <image2>, <image3>, <image4>). Each individual audio can focus on a subset of images, but collectively they must explain all 4 images.

[CRITICAL STYLE REQUIREMENTS]
- The audio text should be written in a natural, flowing narrative style, NOT in a mechanical or template-like format
- AVOID starting audio descriptions with phrases like "In the audio, the dish in <image1> is..." or "Moving to <image2>..." 
- Instead, write the audio text as if it's a natural, engaging description that flows smoothly from one image to another
- Use varied sentence structures and natural transitions between images
- The audio should sound like it's coming from a knowledgeable food expert giving a natural explanation, not from a rigid template

[Example of GOOD audio style (like 0001):]
"The dishes presented in the images highlight diverse culinary traditions. Beginning with <image1>, this hot pot showcases a variety of ingredients simmering together, reflecting communal dining practices commonly seen in East Asian cultures. Moving on to <image2>, the noodle dish emphasizes the simplicity and freshness of vegetables combined with noodles, a staple in many Asian cuisines..."

[Example of BAD audio style (like 0002):]
"In the audio, the dish in <image1> is a hot pot featuring tofu, mushrooms, and corn, highlighting its nourishing qualities. Moving to <image2>, the noodles served with vegetables create a hearty and warming meal..."

[IMPORTANT CLARIFICATION]
The number of audios (e.g., 2 or 3) indicates how many detailed audio explanations you should provide. Each audio can focus on different images, but ALL audios together must comprehensively cover ALL 4 images. For example:
- If you need 2 audios: Audio1 explains images 1&2, Audio2 explains images 3&4 (covers all 4 images)
- If you need 3 audios: Audio1 explains image 1, Audio2 explains images 2&3, Audio3 explains image 4 (covers all 4 images but the distribution of the images can be random)

[Example of correct understanding]
Question asks about 4 images and requests 2 audios ‚Üí Answer should describe ALL 4 images across BOTH audios combined, not necessarily each audio covering all images.

[Output Content Requirement]
The output content (golden annotation answer) MUST reference all audio tags using <audio1>, <audio2>, etc. format to create a cohesive narrative that connects all the audio explanations together.

[Output Modal Requirement]
The output modal (audio text) MUST reference all image tags using <image1>, <image2>, <image3>, <image4> format to indicate which specific images each audio explains. This ensures clear mapping between audios and images.

Please respond with only the JSON, no additional text or explanation.


    """

    return prompt

def process_single_id(data_id: str, delay: int = 2) -> bool:
    """
    Â§ÑÁêÜÂçï‰∏™IDÁöÑQAÂØπÁîüÊàê
    
    Args:
        data_id: Êï∞ÊçÆIDÔºåÂ¶Ç "0001"
        delay: APIË∞ÉÁî®Èó¥ÈöîÂª∂Ëøü(Áßí)
        
    Returns:
        ÊòØÂê¶ÊàêÂäü
    """
    print(f"\nüîÑ Â§ÑÁêÜID: {data_id}")
    
    # ÁîüÊàêÂõæÁâáURL
    image_urls = generate_image_urls(data_id)
    
    print(f"   üì∑ ÂõæÁâáURLs:")
    for i, url in enumerate(image_urls, 1):
        print(f"      image{i}: {url}")
    
    # ÂàõÂª∫prompt
    prompt = create_prompt(image_urls, data_id)
    
    # Ë∞ÉÁî®API
    print(f"   üì§ Ë∞ÉÁî®OpenAI API...")
    response = call_openai_api(prompt, image_urls)
    
    if response:
        try:
            # Ê∏ÖÁêÜÂìçÂ∫î
            cleaned_response = response.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]
            cleaned_response = cleaned_response.strip()
            
            qa_pair = json.loads(cleaned_response)
            
            # ‰øùÂ≠òÂà∞JSONLÊñá‰ª∂
            output_file = "generated_qa_pairs.jsonl"
            with open(output_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(qa_pair, ensure_ascii=False, indent=2) + "\n")
            
            print(f"   ‚úÖ ÊàêÂäüÁîüÊàêQAÂØπÔºåÂ∑≤‰øùÂ≠ò")
            return True
            
        except json.JSONDecodeError as e:
            print(f"   ‚ùå JSONËß£ÊûêÂ§±Ë¥•: {e}")
            # ‰øùÂ≠òÂéüÂßãÂìçÂ∫îÂà∞ÈîôËØØÊó•Âøó
            with open("error_log.txt", "a", encoding="utf-8") as f:
                f.write(f"ID: {data_id}\nError: {e}\nResponse: {response}\n{'='*50}\n")
            return False
            
    else:
        print(f"   ‚ùå APIË∞ÉÁî®Â§±Ë¥•")
        # ËÆ∞ÂΩïÈîôËØØ
        with open("error_log.txt", "a", encoding="utf-8") as f:
            f.write(f"ID: {data_id}\nError: APIË∞ÉÁî®Â§±Ë¥•\n{'='*50}\n")
        return False

def batch_process(start_id: int = 1, end_id: int = 10, delay: int = 2):
    """
    ÊâπÈáèÂ§ÑÁêÜÂ§ö‰∏™ID
    
    Args:
        start_id: Ëµ∑ÂßãID
        end_id: ÁªìÊùüID
        delay: APIË∞ÉÁî®Èó¥ÈöîÂª∂Ëøü(Áßí)
    """
    print("üöÄ FoodieQA ÊâπÈáèÁîüÊàêQAÂØπËÑöÊú¨")
    print("=" * 50)
    print(f"üéØ Â§ÑÁêÜËåÉÂõ¥: ID {start_id:04d} Âà∞ {end_id:04d}")
    print(f"‚è±Ô∏è  APIÂª∂Ëøü: {delay}Áßí")
    print("=" * 50)
    
    # Ê£ÄÊü•APIÂØÜÈí•
    if not API_KEY:
        print("‚ùå ËØ∑Âú®config.envÊñá‰ª∂‰∏≠ËÆæÁΩÆOPENAI_API_KEYÁéØÂ¢ÉÂèòÈáè")
        return
    
    # ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï
    output_dir = "generated_qa_data"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    success_count = 0
    error_count = 0
    
    # ÊâπÈáèÂ§ÑÁêÜ
    for i in range(start_id, end_id + 1):
        data_id = f"{i:04d}"  # Ê†ºÂºèÂåñ‰∏∫4‰ΩçÊï∞Â≠óÔºåÂ¶Ç "0001"
        
        success = process_single_id(data_id, delay)
        if success:
            success_count += 1
        else:
            error_count += 1
        
        # Ê∑ªÂä†Âª∂Ëøü‰ª•ÈÅøÂÖçAPIÈôêÂà∂ÔºàÈô§‰∫ÜÊúÄÂêé‰∏Ä‰∏™Ôºâ
        if i < end_id:
            print(f"   ‚è≥ Á≠âÂæÖ{delay}ÁßíÂêéÁªßÁª≠...")
            time.sleep(delay)
    
    # ËæìÂá∫ÁªüËÆ°‰ø°ÊÅØ
    print(f"\n{'='*50}")
    print(f"üìä Â§ÑÁêÜÂÆåÊàê!")
    print(f"‚úÖ ÊàêÂäü: {success_count} ‰∏™")
    print(f"‚ùå Â§±Ë¥•: {error_count} ‰∏™")
    print(f"üìÅ ËæìÂá∫Êñá‰ª∂: generated_qa_pairs.jsonl")
    if error_count > 0:
        print(f"üìù ÈîôËØØÊó•Âøó: error_log.txt")
    print(f"{'='*50}")

def main():
    """‰∏ªÂáΩÊï∞"""
    print("FoodieQA ÊâπÈáèÁîüÊàêQAÂØπËÑöÊú¨")
    print("=" * 50)
    print("ËØ∑ÈÄâÊã©Ê®°Âºè:")
    print("1. ÂçïÊ¨°Â§ÑÁêÜ (ÊµãËØïÁî®)")
    print("2. ÊâπÈáèÂ§ÑÁêÜ")
    
    choice = input("ËØ∑ËæìÂÖ•ÈÄâÊã© (1Êàñ2): ").strip()
    
    if choice == "1":
        # ÂçïÊ¨°Â§ÑÁêÜÊ®°Âºè
        print("\nüéØ ÂçïÊ¨°Â§ÑÁêÜÊ®°Âºè")
        data_id = input("ËØ∑ËæìÂÖ•Ë¶ÅÂ§ÑÁêÜÁöÑID (‰æãÂ¶Ç: 0001): ").strip()
        if not data_id:
            data_id = "0001"
        
        process_single_id(data_id)
        
    elif choice == "2":
        # ÊâπÈáèÂ§ÑÁêÜÊ®°Âºè
        print("\nüìä ÊâπÈáèÂ§ÑÁêÜÊ®°Âºè")
        start_id = input("ËØ∑ËæìÂÖ•Ëµ∑ÂßãID (ÈªòËÆ§: 1): ").strip()
        end_id = input("ËØ∑ËæìÂÖ•ÁªìÊùüID (ÈªòËÆ§: 10): ").strip()
        delay = input("ËØ∑ËæìÂÖ•APIÂª∂ËøüÁßíÊï∞ (ÈªòËÆ§: 2): ").strip()
        
        # ËÆæÁΩÆÈªòËÆ§ÂÄº
        start_id = int(start_id) if start_id.isdigit() else 1
        end_id = int(end_id) if end_id.isdigit() else 10
        delay = int(delay) if delay.isdigit() else 2
        
        batch_process(start_id, end_id, delay)
        
    else:
        print("‚ùå Êó†ÊïàÈÄâÊã©")

if __name__ == "__main__":
    main()