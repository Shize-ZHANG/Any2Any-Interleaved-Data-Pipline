#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡ç”ŸæˆQAå¯¹çš„è„šæœ¬
è‡ªåŠ¨æ›´æ¢URLä¸­çš„æ–‡ä»¶åï¼Œæ‰¹é‡å¤„ç†å¤šä¸ªID
"""

import json
import os
import time
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('config.env')

# ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
API_KEY = os.getenv('OPENAI_API_KEY')

def generate_image_urls(data_id: str) -> list:
    """
    æ ¹æ®æ•°æ®IDç”Ÿæˆå¯¹åº”çš„å›¾ç‰‡URLåˆ—è¡¨
    
    Args:
        data_id: æ•°æ®IDï¼Œå¦‚ "0001", "0002" ç­‰
        
    Returns:
        åŒ…å«4å¼ å›¾ç‰‡URLçš„åˆ—è¡¨
    """
    base_url = "https://raw.githubusercontent.com/liyanlin06/any2any_data/main/general_area/food/image"
    
    # ç”Ÿæˆ4å¼ å›¾ç‰‡çš„URLï¼Œæ–‡ä»¶åæ ¼å¼ï¼šimg_{data_id}_{åºå·}.{æ‰©å±•å}
    image_urls = [
        f"{base_url}/img_{data_id}_01.jpg",
        f"{base_url}/img_{data_id}_02.jpeg", 
        f"{base_url}/img_{data_id}_03.jpeg",
        f"{base_url}/img_{data_id}_04.jpeg"
    ]
    
    return image_urls

def call_openai_api(prompt: str, image_urls: list) -> str:
    """
    è°ƒç”¨OpenAI APIï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥
    
    Args:
        prompt: å‘é€çš„æ–‡æœ¬prompt
        image_urls: å›¾ç‰‡URLåˆ—è¡¨
        
    Returns:
        APIè¿”å›çš„å“åº”
    """
    client = OpenAI(api_key=API_KEY)
    
    try:
        # æ„å»ºæ¶ˆæ¯å†…å®¹ï¼ŒåŒ…å«æ–‡æœ¬å’Œå›¾ç‰‡
        content = [{"type": "text", "text": prompt}]
        
        # æ·»åŠ å›¾ç‰‡URLåˆ°æ¶ˆæ¯å†…å®¹ä¸­
        for url in image_urls[:4]:  # æœ€å¤š4å¼ å›¾ç‰‡
            content.append({
                "type": "image_url", 
                "image_url": {"url": url}
            })
        
        response = client.chat.completions.create(
            model="gpt-4o",  # GPT-4o æ¨¡å‹
            messages=[
                {
                    "role": "system", 
                    "content": "You are a multimodal expert. Generate structured JSON data for multimodal question-answer pairs. Always respond with properly formatted, multi-line JSON that is easy to read."
                },
                {
                    "role": "user", 
                    "content": content
                }
            ],
            max_tokens=5000,
            response_format={"type": "json_object"}  # å¼ºåˆ¶è¾“å‡ºJSONæ ¼å¼
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"âŒ APIè°ƒç”¨å¤±è´¥: {str(e)}")
        return None

def create_prompt(image_urls: list, data_index: int = 1) -> str:
    """
    åˆ›å»ºæ‚¨æä¾›çš„prompt
    
    Args:
        image_urls: å›¾ç‰‡URLåˆ—è¡¨
        data_index: æ•°æ®ç´¢å¼•
        
    Returns:
        å®Œæ•´çš„prompt
    """
    # æ„å»ºåŸå§‹æ•°æ®
    original_data = {}
    for i, url in enumerate(image_urls[:4]):  # æœ€å¤š4å¼ å›¾ç‰‡
        original_data[f"image{i+1}"] = url
    
    prompt = f"""You are a multimodal expert. Based on the following original data, please construct a data (Question-Answer pair) entry that strictly conforms to the JSON format below.
Please design a multimodal interleaved Question-Answer pair. You can place different pieces of information from the original data into the input or output of the Question-Answer pair.

[Original data]
{json.dumps(original_data, indent=2, ensure_ascii=False)}

[Question-Answer pair JSON template]
This Question-Answer pair must adhere to the following structure in the following JSON template and don't generate additional information.
{{
    "domain": "general_domain",
    "subdomain": "food",
    "id": "{data_index}",
    "input": {{
        "modal": {{
            "image1": "url",
            "image2": "url",
            "image3": "url",
            "image4": "url"
        }},
        "content": "Must interleave <image1>, <image2>, <image3>, <image4> tags at the appropriate position in the text and clearly indicate that the answer must include the number of audios in the output to support or illustrate the explanation, which means that the question should clearly indicate that the answer must include the number of audios in the output to support or illustrate the explanation. "
    }},
    "output": {{
        "modal": {{
            "audio1": "text",
            ...
        }},
        "content": "This is the golden annotation answer that the model is expected to generate. Your answer text MUST naturally integrate all <audio> tags at appropriate positions with the format of <audio1>, <audio2>, etc. within the text. IMPORTANT: All audios together should provide comprehensive explanation for ALL images, with each audio potentially focusing on different subsets of images. The content should reference each audio using <audio1>, <audio2>, etc. format to create a cohesive narrative."
    }}
}}

[Construction requirements]
1 You need to design appropriate question-answer pair and clearly indicate in the question which specific modalities other than text are required to be included in the answer.
2 The output content MUST reference all audio tags using <audio1>, <audio2>, etc. format to create a cohesive narrative.
3 The output modal (audio text) MUST reference all image tags using <image1>, <image2>, <image3>, <image4> format to indicate which images each audio explains. 
2 The content of the input is the entire input fed into the model. The question-answer pair should be open-world QA. 
3 The content of the input is the entire input fed into the model and the content of the output is the golden output of the model. You should design the input content and output content based on the original data.
4 The content of the input MUST contain all tags corresponding to the input modality using <image1>, <image2>, <image3>, <image4> format, and the content of the output MUST contain all tags corresponding to the output modality using <audio1>, <audio2>, etc. format.
5 Give the JSON directly, no additional output information.
6 The <> tags should be the components of the text sentence, not just a single word. For example, the <> tags can serve as the subject, object, or other components of the sentence.
7 Please note that the <> tags of the input should not appear in the output.
8 You should design the text of the <audio> and fill the text into the "text" position of the <audio>. Each audio text MUST reference the specific images it explains using <image1>, <image2>, <image3>, <image4> tags.
9 IMPORTANT: The number of <audio> in the output is 1. If the number of <audio> is more than 1, each audio can focus on different images, but all audios together must cover ALL images.
10 IMPORTANT: The output must contain exactly the same number of audios as specified in the input question with the format of <audio1>, <audio2>, etc. Remember: audios collectively explain all images, with each audio potentially focusing on different subsets. 
12 IMPORTANT: The output content MUST not contain the <image> tags of images in the output. The output content MUST reference all audio tags using <audio1>, <audio2>, etc. format to create a cohesive narrative that ties together all the audio explanations.
13 IMPORTANT: All audios together MUST cover ALL images (<image1>, <image2>, <image3>, <image4>). Each individual audio can focus on a subset of images, but collectively they must explain all 4 images.

[IMPORTANT CLARIFICATION]
The number of audios (e.g., 2 or 3) indicates how many detailed audio explanations you should provide. Each audio can focus on different images, but ALL audios together must comprehensively cover ALL 4 images. For example:
- If you need 2 audios: Audio1 explains images 1&2, Audio2 explains images 3&4 (covers all 4 images)
- If you need 3 audios: Audio1 explains image 1, Audio2 explains images 2&3, Audio3 explains image 4 (covers all 4 images but the distribution of the images can be random)

[Example of correct understanding]
Question asks about 4 images and requests 2 audios â†’ Answer should describe ALL 4 images across BOTH audios combined, not necessarily each audio covering all images.

[Output Content Requirement]
The output content (golden annotation answer) MUST reference all audio tags using <audio1>, <audio2>, etc. format to create a cohesive narrative that connects all the audio explanations together.

[Output Modal Requirement]
The output modal (audio text) MUST reference all image tags using <image1>, <image2>, <image3>, <image4> format to indicate which specific images each audio explains. This ensures clear mapping between audios and images.

Please respond with only the JSON, no additional text or explanation.


    """

    return prompt

def process_single_id(data_id: str, delay: int = 2) -> bool:
    """
    å¤„ç†å•ä¸ªIDçš„QAå¯¹ç”Ÿæˆ
    
    Args:
        data_id: æ•°æ®IDï¼Œå¦‚ "0001"
        delay: APIè°ƒç”¨é—´éš”å»¶è¿Ÿ(ç§’)
        
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    print(f"\nğŸ”„ å¤„ç†ID: {data_id}")
    
    # ç”Ÿæˆå›¾ç‰‡URL
    image_urls = generate_image_urls(data_id)
    
    print(f"   ğŸ“· å›¾ç‰‡URLs:")
    for i, url in enumerate(image_urls, 1):
        print(f"      image{i}: {url}")
    
    # åˆ›å»ºprompt
    prompt = create_prompt(image_urls, data_id)
    
    # è°ƒç”¨API
    print(f"   ğŸ“¤ è°ƒç”¨OpenAI API...")
    response = call_openai_api(prompt, image_urls)
    
    if response:
        try:
            # æ¸…ç†å“åº”
            cleaned_response = response.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]
            cleaned_response = cleaned_response.strip()
            
            qa_pair = json.loads(cleaned_response)
            
            # ä¿å­˜åˆ°JSONLæ–‡ä»¶
            output_file = "generated_qa_pairs.jsonl"
            with open(output_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(qa_pair, ensure_ascii=False, indent=2) + "\n")
            
            print(f"   âœ… æˆåŠŸç”ŸæˆQAå¯¹ï¼Œå·²ä¿å­˜")
            return True
            
        except json.JSONDecodeError as e:
            print(f"   âŒ JSONè§£æå¤±è´¥: {e}")
            # ä¿å­˜åŸå§‹å“åº”åˆ°é”™è¯¯æ—¥å¿—
            with open("error_log.txt", "a", encoding="utf-8") as f:
                f.write(f"ID: {data_id}\nError: {e}\nResponse: {response}\n{'='*50}\n")
            return False
            
    else:
        print(f"   âŒ APIè°ƒç”¨å¤±è´¥")
        # è®°å½•é”™è¯¯
        with open("error_log.txt", "a", encoding="utf-8") as f:
            f.write(f"ID: {data_id}\nError: APIè°ƒç”¨å¤±è´¥\n{'='*50}\n")
        return False

def batch_process(start_id: int = 1, end_id: int = 10, delay: int = 2):
    """
    æ‰¹é‡å¤„ç†å¤šä¸ªID
    
    Args:
        start_id: èµ·å§‹ID
        end_id: ç»“æŸID
        delay: APIè°ƒç”¨é—´éš”å»¶è¿Ÿ(ç§’)
    """
    print("ğŸš€ FoodieQA æ‰¹é‡ç”ŸæˆQAå¯¹è„šæœ¬")
    print("=" * 50)
    print(f"ğŸ¯ å¤„ç†èŒƒå›´: ID {start_id:04d} åˆ° {end_id:04d}")
    print(f"â±ï¸  APIå»¶è¿Ÿ: {delay}ç§’")
    print("=" * 50)
    
    # æ£€æŸ¥APIå¯†é’¥
    if not API_KEY:
        print("âŒ è¯·åœ¨config.envæ–‡ä»¶ä¸­è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "generated_qa_data"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    success_count = 0
    error_count = 0
    
    # æ‰¹é‡å¤„ç†
    for i in range(start_id, end_id + 1):
        data_id = f"{i:04d}"  # æ ¼å¼åŒ–ä¸º4ä½æ•°å­—ï¼Œå¦‚ "0001"
        
        success = process_single_id(data_id, delay)
        if success:
            success_count += 1
        else:
            error_count += 1
        
        # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
        if i < end_id:
            print(f"   â³ ç­‰å¾…{delay}ç§’åç»§ç»­...")
            time.sleep(delay)
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*50}")
    print(f"ğŸ“Š å¤„ç†å®Œæˆ!")
    print(f"âœ… æˆåŠŸ: {success_count} ä¸ª")
    print(f"âŒ å¤±è´¥: {error_count} ä¸ª")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: generated_qa_pairs.jsonl")
    if error_count > 0:
        print(f"ğŸ“ é”™è¯¯æ—¥å¿—: error_log.txt")
    print(f"{'='*50}")

def main():
    """ä¸»å‡½æ•°"""
    print("FoodieQA æ‰¹é‡ç”ŸæˆQAå¯¹è„šæœ¬")
    print("=" * 50)
    print("è¯·é€‰æ‹©æ¨¡å¼:")
    print("1. å•æ¬¡å¤„ç† (æµ‹è¯•ç”¨)")
    print("2. æ‰¹é‡å¤„ç†")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1æˆ–2): ").strip()
    
    if choice == "1":
        # å•æ¬¡å¤„ç†æ¨¡å¼
        print("\nğŸ¯ å•æ¬¡å¤„ç†æ¨¡å¼")
        data_id = input("è¯·è¾“å…¥è¦å¤„ç†çš„ID (ä¾‹å¦‚: 0001): ").strip()
        if not data_id:
            data_id = "0001"
        
        process_single_id(data_id)
        
    elif choice == "2":
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        print("\nğŸ“Š æ‰¹é‡å¤„ç†æ¨¡å¼")
        start_id = input("è¯·è¾“å…¥èµ·å§‹ID (é»˜è®¤: 1): ").strip()
        end_id = input("è¯·è¾“å…¥ç»“æŸID (é»˜è®¤: 10): ").strip()
        delay = input("è¯·è¾“å…¥APIå»¶è¿Ÿç§’æ•° (é»˜è®¤: 2): ").strip()
        
        # è®¾ç½®é»˜è®¤å€¼
        start_id = int(start_id) if start_id.isdigit() else 1
        end_id = int(end_id) if end_id.isdigit() else 10
        delay = int(delay) if delay.isdigit() else 2
        
        batch_process(start_id, end_id, delay)
        
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main()